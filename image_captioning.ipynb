{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"image_captioning.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"a5sw5yWvgp58","colab_type":"text"},"source":["# Generation of Captions from Images"]},{"cell_type":"markdown","metadata":{"id":"wbGzHZ3ww3nO","colab_type":"text"},"source":["Tianyuan Cai"]},{"cell_type":"markdown","metadata":{"id":"Z6JnvN-hHmve","colab_type":"text"},"source":["**Documentation Requirements:**\n","\n","**Project summary (Abstract)**\n","\n","One page Word document in a separate file. Project summary should contain your name /team,  title of your project and URLs to YouTube video.\n","Produce a one page summary to describe the problem you are trying to solve, your data set, the particular technology topic or feature you are demonstrating, its uses, benefits, drawbacks, challenges and your results.  Describe briefly the working example you prepared. This first page is very important. Based on that page, your colleagues will judge whether to download your project and spend time reading your documentation and code. Please add YouTube / reference URLs  to the bottom of the one page summary. \n","\n","Filenames should be prefixed with team if appropriate and then topic (abbreviate project), followed by your name: \n","\t- you must identify and list all team members in all  of your FP files\n","\t- you must prefix your final zip with label \"Team\" e.g. Team_ShortTopicName_YourLastNameYourFirstName_Final.zip\n","\n","**Demo/solution implementation and working code (python notebook)**\n","\n","Produce a single working demo that meets your problem statement and provides a full implementation of your solution.  You may not simply copy examples (demonstrations) of the technology that are online. (Grade will be 0).  You can use code skeletons that are provided as we have done in class and show your extended programming use of them.  Provide neatly organized and complete working code with comments. Please note that a project will not be given any points if a working implementation and code are missing. Please provide the URL to your full data source. If your data set is larger than 10MB, PLEASE DO NOT UPLOAD your data set. In such a case, please provide only a sample of your data with the rest of your submission. Our site has a limited capacity and a few large submissions could block it.\n","PLEASE, PLEASE do not upload final projects which take more than 20 MB of space. In the past we had students who would upload GBs of data and block Canvas site. We do not find such practical jokes amusing.\n","\n","**Slides**\n","\n","Produce a set of Power Point slides (10-20) with a few snapshots of your demo which captures the key points: your problem statement, what the technology does, your demonstration and pros/cons. The first page of your Power Point slide must have a standard format that we provide.  Please use white background for all your Power Point slides. White background makes slides readable, printable and presentable on YouTube. Place URLs of your YouTube videos on the last slide.  Note: The filename should start with the topic name followed by your name\n","\n","**Report**\n","\n","Produce a detailed document with a complete description of analyzed technology or use case including all installation and configuration steps. Your report will start with your name and project title. Detailed installations and configuration is required. Your colleagues must be able to reproduce your work based solely on the steps you have documented.  Describe your problem statement, data set, installation/configuration, results, what worked, what did not and why not, and any lessons learned. Report must show all steps to reproduce examples that you developed.  This report is similar to your homework solutions where all steps are described along with the results.\n","* The first page of your report should be the same as the one page summary (abstract). The first page should contain your name /  team (as appropriate list all team members) and the project topic name. \n","* Please include your name and your project topic in the header or the footer of your MS Word Document. \n","* Name your file like you named 1 page summary appended with `_report`.\n","* Please include page numbers. \n","* You are welcome to upload a PDF version of all documents. However, you must always submit an MS Word version of your report. Just submitting a Jupyter notebook is not acceptable. You must create a written report in MS Word.\n","* Please include URLs for your YouTube videos at the end of your report\n","\n","**2-minute YouTube presentation**\n","\n","Produce a 2 minute YouTube presentation. This video will contain a summary of the technology and a quick overview of your demonstration. This video might get presented to the entire class so please make sure it is 2 minutes and not more. \n","\n","**Submission Instructions: **\n","\n","`Image_Captioning_CaiTianyuan_Final.zip`\n","* Submit final project on the course site. \n","* Files to be submitted\n","\t- Report, by itself\n","\t- Slides, by itself\n","\t- Code and surrounding artifacts, in a ZIP file\n","\t\t- viz in a separate folder\n","\t- Data files, in a separate ZIP file\n","\n","Grading criteria:\n","If you fail to provide working code submitted in working directories with all artifacts it produces and (toy-sample) dataset, your project will not be graded. \n","\n","All other artifacts: Report, One page summary, PPTs are uploaded separately.\n","\n","Assuming code is submitted:\n","* 50%, Project Report and practical software code example\n","* 25%, PowerPoint Slides\n","* 15%, 2 minute YouTube video\n","* 10%, One page summary"]},{"cell_type":"markdown","metadata":{"id":"ZxUH3z1rg3h2","colab_type":"text"},"source":["## Project Summary\n","### Overview\n","In this project, I ...\n","\n","Image captioning has a large variety of applications. By understanding common objects in an image, and transforming them into text information, we can compress large photographic infomation into more compact data formats. This ability can be applied in a wide range of fields such as voice over technologies, product recommendation, etc. \n","\n","### Data\n","\n","Common Objects in Context (commonly referred to as [COCO](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb)) data set provided by Microsoft is used.\n","\n","COCO dataset provides a set of images that identifies common objects in context. The data include both images and captions that describes the objects and their context.\n","\n","If I had more time, include flickr data.\n","\n","### Methodology\n","\n","#### Model\n","\n","Usage, pros and cons\n","\n","#### Techniques\n","\n","Usage, pros and cons\n","\n","* Hyper-parameter tuning\n","* InceptionV3 trained on ImageNet, tradeoffs and alternatives\n","* Glove\n","\n","### The rest of the document\n","\n","* Working code\n","* Examples"]},{"cell_type":"markdown","metadata":{"id":"j0DEAXPRIfdQ","colab_type":"text"},"source":["## Problem Statement"]},{"cell_type":"markdown","metadata":{"id":"7wWA0QcgIomW","colab_type":"text"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"ztrAd6kjfhJ5","colab_type":"text"},"source":["## Preparing Environment"]},{"cell_type":"markdown","metadata":{"id":"vEfOhAQjIVGY","colab_type":"text"},"source":["_[Detailed installations and configuration is required.]_"]},{"cell_type":"markdown","metadata":{"id":"b7cU2qwpfhJ5","colab_type":"text"},"source":["I start by installing packages that do not commonly come with Anaconda. This particular program uses tensorflow-gpu on a Titan Xp GPU. The `tqdm` package might be needed when loading encoded images on a computer with limited memory. "]},{"cell_type":"markdown","metadata":{"id":"GbjijHmvfhJ6","colab_type":"text"},"source":["Download necessary packages and dependencies."]},{"cell_type":"code","metadata":{"id":"-Z5eMvabfhJ6","colab_type":"code","colab":{}},"source":["# !pip install pillow numpy scikit-image keras Cython pycocotools nltk\n","\n","# import nltk\n","# nltk.download('stopwords')\n","# nltk.download('wordnet')\n","# nltk.download('punkt')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-dSz2wp1fhJ-","colab_type":"code","colab":{}},"source":["import os\n","import pickle\n","from time import time\n","import json\n","import re\n","\n","from pycocotools.coco import COCO\n","import sklearn\n","import keras\n","import keras.applications.imagenet_utils\n","from PIL import Image\n","import tensorflow as tf\n","\n","import skimage.io as io\n","import seaborn as sns\n","\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","import numpy as np\n","np.random.seed(0)\n","import warnings\n","warnings.filterwarnings('ignore')\n","import pylab\n","pylab.rcParams['figure.figsize'] = (4, 6)\n","from keras.applications.inception_v3 import preprocess_input\n","\n","import nltk\n","from nltk.stem import WordNetLemmatizer"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"HcuNaWygfhKA","colab_type":"code","outputId":"4d0da7b0-ee3d-4e3a-c4d3-107fbb2fa9c2","executionInfo":{"status":"ok","timestamp":1564346775450,"user_tz":240,"elapsed":1029,"user":{"displayName":"Tianyuan Cai","photoUrl":"https://lh3.googleusercontent.com/-C5v21Di0WHU/AAAAAAAAAAI/AAAAAAAB5u0/9y6s_YNrJUU/s64/photo.jpg","userId":"13228707440502595479"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Test if running on GPU\n","keras.backend.tensorflow_backend._get_available_gpus()"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['/job:localhost/replica:0/task:0/device:GPU:0']"]},"metadata":{"tags":[]},"execution_count":66}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"vxPXfeSwEZIo"},"source":["## Data"]},{"cell_type":"markdown","metadata":{"id":"N6DVeX2RoyDo","colab_type":"text"},"source":["### Data Overview"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TrwlBEz-E6NU"},"source":["The data source used for this analysis is called COCO, common objects in context. See [Coco API](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb) for package usage. \n","\n","COCO dataset provides a set of images that identifies common objects in context. The data include both images and captions that describes the objects and their context."]},{"cell_type":"markdown","metadata":{"id":"yjtrF1VafhKF","colab_type":"text"},"source":["We start by extracting image and caption (annotation) data from COCO website. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HixAVIUzygZ-","colab":{}},"source":["data_dir = \"/content\"\n","data_type = \"train2017\"\n","data_zipfile = '%s.zip'%(data_type)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytha400S00x7","colab_type":"code","colab":{}},"source":["# annotation_zip = tf.keras.utils.get_file('captions.zip',\n","#                                          cache_subdir=os.path.abspath('.'),\n","#                                          origin = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n","#                                          extract = True)\n","# image_zip = tf.keras.utils.get_file(data_zipfile,\n","#                                     cache_subdir=os.path.abspath('.'),\n","#                                     origin = 'http://images.cocodataset.org/zips/%s'%(data_zipfile),\n","#                                     extract = True)\n","# glove6b_zip = tf.keras.utils.get_file('glove.6B.zip',\n","#                                     cache_subdir=os.path.abspath('./glove6b'),\n","#                                     origin = 'http://nlp.stanford.edu/data/glove.6B.zip',\n","#                                     extract = True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"XabFXwZ701EI","colab_type":"code","colab":{}},"source":["# Annotation file\n","annotation_file = data_dir + '/annotations/captions_%s.json'%(data_type)\n","image_dir = data_dir+'/%s/'%(data_type)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovCgXdJ-fhKH","colab_type":"text"},"source":["Coco provides images that are categorized into a variety of categories. The categories listed below can be used to index images in the data set."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jchnZh2iFt1F","outputId":"8cc41222-d19b-46af-9c6c-6ab28b7c6c08","executionInfo":{"status":"ok","timestamp":1564346799587,"user_tz":240,"elapsed":25117,"user":{"displayName":"Tianyuan Cai","photoUrl":"https://lh3.googleusercontent.com/-C5v21Di0WHU/AAAAAAAAAAI/AAAAAAAB5u0/9y6s_YNrJUU/s64/photo.jpg","userId":"13228707440502595479"}},"colab":{"base_uri":"https://localhost:8080/","height":275}},"source":["# Initialize COCO api for instance annotations\n","coco_caps=COCO(annotation_file)\n","\n","# Obtain categories\n","annFile='{}/annotations/instances_{}.json'.format(data_dir, data_type)\n","coco=COCO(annFile)\n","\n","cats = coco.loadCats(coco.getCatIds())\n","nms=[cat['name'] for cat in cats]\n","print('\\nCOCO Categories: \\n{}\\n'.format(' '.join(nms)))\n","\n","nms = set([cat['supercategory'] for cat in cats])\n","print('COCO Supercategories: \\n{}'.format(' '.join(nms)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["loading annotations into memory...\n","Done (t=1.38s)\n","creating index...\n","index created!\n","loading annotations into memory...\n","Done (t=17.76s)\n","creating index...\n","index created!\n","\n","COCO Categories: \n","person bicycle car motorcycle airplane bus train truck boat traffic light fire hydrant stop sign parking meter bench bird cat dog horse sheep cow elephant bear zebra giraffe backpack umbrella handbag tie suitcase frisbee skis snowboard sports ball kite baseball bat baseball glove skateboard surfboard tennis racket bottle wine glass cup fork knife spoon bowl banana apple sandwich orange broccoli carrot hot dog pizza donut cake chair couch potted plant bed dining table toilet tv laptop mouse remote keyboard cell phone microwave oven toaster sink refrigerator book clock vase scissors teddy bear hair drier toothbrush\n","\n","COCO Supercategories: \n","accessory food animal indoor person kitchen sports furniture outdoor vehicle appliance electronic\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"mDJCZrLuFg1A","outputId":"2a399138-b938-480b-f4fd-280fa6cd1fc4","executionInfo":{"status":"error","timestamp":1564346799908,"user_tz":240,"elapsed":25430,"user":{"displayName":"Tianyuan Cai","photoUrl":"https://lh3.googleusercontent.com/-C5v21Di0WHU/AAAAAAAAAAI/AAAAAAAB5u0/9y6s_YNrJUU/s64/photo.jpg","userId":"13228707440502595479"}},"colab":{"base_uri":"https://localhost:8080/","height":494}},"source":["# Show sample data set by choosing categories\n","temp_cat = ['dog','person', 'ball']\n","\n","catIds = coco.getCatIds(catNms=temp_cat)\n","imgIds = coco.getImgIds(catIds=catIds)\n","\n","if len(imgIds) > 0:\n","    imgIds = coco.getImgIds(imgIds = imgIds[-1]) # Pick the last image\n","    print(\"The index of the chosen image is %s.\\n\"%(str(imgIds[0])))\n","else:\n","    print(\"No matched images found.\")\n","\n","# load and display captions\n","annIds = coco_caps.getAnnIds(imgIds)\n","anns = coco_caps.loadAnns(annIds)\n","print(\"The corresponding captions are:\")\n","coco_caps.showAnns(anns)\n","\n","# Show image\n","img = coco.loadImgs(imgIds)[0]\n","I = io.imread('%s/%s/%s'%(data_dir,data_type,img['file_name']))\n","plt.imshow(I); plt.axis('off'); plt.show()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The index of the chosen image is 106484.\n","\n","The corresponding captions are:\n","A man walking a dalmatian on a red leash.\n","The man has a red leash on his Dalmatian dog.\n","a man walks a dog with a leech \n","A guy is walking his dalmatian down the road. \n","A man walking a dalmatian on leash in front of a firetruck.\n"],"name":"stdout"},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-71-e5496cf4bf82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Show image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadImgs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimgIds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mI\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'%s/%s/%s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'file_name'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'off'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/io/_io.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(fname, as_gray, plugin, flatten, **plugin_args)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mfile_or_url_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall_plugin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'imread'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplugin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mplugin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mplugin_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ndim'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/skimage/io/manage_plugins.py\u001b[0m in \u001b[0;36mcall_plugin\u001b[0;34m(kind, *args, **kwargs)\u001b[0m\n\u001b[1;32m    208\u001b[0m                                (plugin, kind))\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mimread\u001b[0;34m(uri, format, **kwargs)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[0;31m# Get reader and read first\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 221\u001b[0;31m     \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"i\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/functions.py\u001b[0m in \u001b[0;36mget_reader\u001b[0;34m(uri, format, mode, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[0;31m# Create request object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m     \u001b[0mrequest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;31m# Get format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, uri, mode, **kwargs)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;31m# Parse what was given\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_uri\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;31m# Set extension\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/imageio/core/request.py\u001b[0m in \u001b[0;36m_parse_uri\u001b[0;34m(self, uri)\u001b[0m\n\u001b[1;32m    271\u001b[0m                 \u001b[0;31m# Reading: check that the file exists (but is allowed a dir)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No such file: '%s'\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    274\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m                 \u001b[0;31m# Writing: check that the directory to write to does exist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: No such file: '/content/train2017/000000106484.jpg'"]}]},{"cell_type":"markdown","metadata":{"id":"pjj7NxeAfhKP","colab_type":"text"},"source":["### Train-test Split"]},{"cell_type":"markdown","metadata":{"id":"sqM4cmTAfhKQ","colab_type":"text"},"source":["Captions, images paths, and encoded image features are needed in order to effectively implemented the training. Rather than using `train_test_split` function, I choose to shuffle the data and subset to the desired number of train and test samples."]},{"cell_type":"code","metadata":{"id":"MNsn4BSmfhKR","colab_type":"code","colab":{}},"source":["# TODO: Change this to a smaller value during submission\n","total_examples = 100000\n","train_examples = int(0.8 * total_examples)\n","\n","# Subset training and testing images\n","with open(annotation_file, 'r') as f:\n","    annotations = json.load(f)\n","\n","all_img_path_vector = {}\n","all_img_name_vector = list(set([x['image_id'] for x in annotations['annotations']]))\n","print(\"Choosing %s training images and %s validation images from a total of %s images\"%(train_examples, total_examples - train_examples, len(all_img_name_vector)))\n","\n","for img_id in all_img_name_vector:\n","    img = coco.loadImgs(img_id)[0]\n","    image_file_path = '%s/%s/%s'%(data_dir,data_type,img['file_name'])\n","    all_img_path_vector[img_id] = image_file_path\n","    \n","all_img_name_vector = sklearn.utils.shuffle(all_img_name_vector, random_state = 0)\n","all_img_name_vector = all_img_name_vector[:total_examples] # subset\n","\n","train_img_name_vector = all_img_name_vector[:train_examples] # train\n","test_img_name_vector = all_img_name_vector[train_examples:] # test"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NAMRwe9NfhKT","colab_type":"text"},"source":["## Extract Image Features"]},{"cell_type":"markdown","metadata":{"id":"O1-c1nYdfhKU","colab_type":"text"},"source":["I use a pre-trained model, Inception V3 model, to encode images such that we can extract the spacial content of the images. I took out the final fully-connected layer in order to pass the features along to the image captioning model. When encoding, note that hash table is used to ensure I am encoding distinct images. This is because one image has multiple captions associated with it. "]},{"cell_type":"markdown","metadata":{"id":"WfsrjoyKfhKU","colab_type":"text"},"source":["### Image Encoder"]},{"cell_type":"code","metadata":{"id":"fwvaNvTjfhKV","colab_type":"code","colab":{}},"source":["# https://keras.io/applications/#inceptionv3\n","# feature_model = keras.applications.inception_v3.InceptionV3(weights='imagenet')\n","image_model = keras.applications.InceptionV3(weights='imagenet')\n","feature_model = keras.models.Model(image_model.input, image_model.layers[-2].output)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Exg9Vg6yfhKa","colab_type":"text"},"source":["Define encoding function that:\n","* Transform the image into consistent sizes\n","* Convert the images to array\n","* Expand dimensions\n","* Preprocess the input\n","* Reshape images"]},{"cell_type":"code","metadata":{"id":"Xc5X8zZNfhKc","colab_type":"code","colab":{}},"source":["# Encoding function for feature extraction\n","def encode(image_path):\n","    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n","    img_array = tf.keras.preprocessing.image.img_to_array(img)\n","    img_array = np.expand_dims(img_array, axis=0)\n","    img_array = keras.applications.inception_v3.preprocess_input(img_array)\n","    fea_vec = feature_model.predict(img_array)\n","    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n","    return(fea_vec)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"l73t3nXAfhKh","colab_type":"code","colab":{}},"source":["# Testing image model\n","def inceptionv3_predict(image_path, image_model):\n","#     image_path = '/home/tcai/Documents/nlp/final_project/train2017/000000262145.jpg'\n","    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n","    img_array = tf.keras.preprocessing.image.img_to_array(img)\n","    img_array = np.expand_dims(img_array, axis=0)\n","    img_array = keras.applications.inception_v3.preprocess_input(img_array)\n","    preds = image_model.predict(img_array)\n","    P = keras.applications.imagenet_utils.decode_predictions(preds)\n","    \n","    # Show prediction result\n","    for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n","        print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n","    \n","    # Show image\n","    I = io.imread(image_path)\n","    plt.imshow(I); plt.axis('off'); plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cSQ0phPGmetf","colab_type":"text"},"source":["The Inception V3 model is able to recoginize objects in a given image. "]},{"cell_type":"code","metadata":{"colab_type":"code","id":"R72kjxv5mdp7","colab":{}},"source":["random_img_id = np.random.choice(all_img_name_vector)\n","random_img_path = all_img_path_vector[random_img_id]\n","inceptionv3_predict(random_img_path, image_model)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CmvtiSAcfhKm","colab_type":"text"},"source":["### Encode Images"]},{"cell_type":"markdown","metadata":{"id":"LGKT0WXZxE6A","colab_type":"text"},"source":["Encode images and then store the encoded image features in a pickle file to be loaded for future uses."]},{"cell_type":"code","metadata":{"id":"be-CQouPfhKn","colab_type":"code","colab":{}},"source":["# # Encode all the train images\n","# start = time()\n","# train_features = {}\n","# for img_id in all_img_name_vector:\n","#     train_features[img_id] = encode(all_img_path_vector[img_id])\n","\n","# print(\"Time taken in seconds =\", time() - start)\n","\n","# # Pickle the features\n","# with open(data_dir + \"/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n","#     pickle.dump(train_features, encoded_pickle)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lZqXI7CufhKq","colab_type":"code","colab":{}},"source":["train_features = pickle.load(open(data_dir + \"/encoded_train_images.pkl\", \"rb\"))\n","print('%d photos in total are encoded. These include both training and testing image set.' % len(train_features))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SG8k_qhAfhKt","colab_type":"text"},"source":["For each captions in the training and testing data set, I append images back onto the captions so that all captions and features are included."]},{"cell_type":"code","metadata":{"id":"ofnkV1n1fhKu","colab_type":"code","colab":{}},"source":["# Train images\n","captions = []\n","images = []\n","images_features = []\n","\n","for img_id in train_img_name_vector:\n","    img_path = all_img_path_vector[img_id]\n","    img_feature = train_features[img_id]\n","    img_captions = coco_caps.loadAnns(coco_caps.getAnnIds(img_id))\n","    \n","    for caption in [x['caption'] for x in img_captions]:\n","        captions.append('start_sentence ' + caption + ' end_sentence')\n","        images.append(img_path)\n","        images_features.append(img_feature)\n","\n","captions, images, images_features = sklearn.utils.shuffle(captions, images, images_features, random_state = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"J16ph71hfhKv","colab_type":"code","colab":{}},"source":["# Test images\n","test_captions = []\n","test_images = []\n","test_images_features = []\n","\n","for img_id in test_img_name_vector:\n","    img_path = all_img_path_vector[img_id]\n","    img_feature = train_features[img_id]\n","    img_captions = coco_caps.loadAnns(coco_caps.getAnnIds(img_id))\n","    \n","    for caption in [x['caption'] for x in img_captions]:\n","        test_captions.append('start_sentence ' + caption + ' end_sentence')\n","        test_images.append(img_path)\n","        test_images_features.append(img_feature)\n","\n","test_captions, test_images, test_images_features = sklearn.utils.shuffle(test_captions, \n","                                                                         test_images, \n","                                                                         test_images_features, \n","                                                                         random_state = 0)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NIL2J-xafhKx","colab_type":"code","colab":{}},"source":["print('Training: %s distinct images, %s captions,'%(len(list(set(images))), \n","                                                    len(captions)))\n","print('Validation: %s distinct images, %s captions,'%(len(list(set(test_images))),\n","                                                      len(test_captions)))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KTahEk0CwPf4"},"source":["## Word Embeddings"]},{"cell_type":"markdown","metadata":{"id":"Bowu5byLfhK3","colab_type":"text"},"source":["### Distribution of Words"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8gzJIDNXwPC0","colab":{}},"source":["# Find the maximum length of any caption in our dataset\n","def calc_max_length(tensor):\n","    return max(len(t) for t in tensor)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"7MDWQEdwwl26","colab":{}},"source":["number_of_words = 6000\n","# Choose the top words from the vocabulary\n","tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words= number_of_words, oov_token=\"<unk>\", \n","                                                  filters='!\"#$%&()*+.,-/:;=?@[\\]^`{|}~ ')\n","captions = [caption.lower() for caption in captions]\n","tokenizer.fit_on_texts(captions)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lDsqxthy5u9V","colab_type":"code","colab":{}},"source":["tokenizer.word_index['<pad>'] = 0\n","tokenizer.index_word[0] = '<pad>'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"8hEjBeBe32IJ","colab_type":"code","colab":{}},"source":["sentence_lengths = [len(caption) for caption in captions]\n","plt.title('Distribution of Caption Lengths')\n","sns.distplot(sentence_lengths)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Eg6HIPwmqhQO"},"source":["### Vector Representations with GloVe"]},{"cell_type":"markdown","metadata":{"id":"EeoLnXVQqh0p","colab_type":"text"},"source":["Global Vectors for Word Representatiion (GloVe) is used to obtain embedding vectors for each word that appear in the captions."]},{"cell_type":"code","metadata":{"id":"dNS6tItWfhLJ","colab_type":"code","colab":{}},"source":["# Calculates sentence and vocab lengths\n","# max_length = calc_max_length(train_seqs)\n","max_length = 100   # This can be changed later\n","vocab_size = len(tokenizer.index_word)\n","print(\"Original max sentence length is %s; I set it to %s.\"%(calc_max_length(captions), max_length))\n","print(\"The vocabulary size is: %s\"%(vocab_size))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"GqAzqESLfhLL","colab_type":"code","colab":{}},"source":["embedding_dim = 300\n","embeddings_index = {}\n","\n","with open(data_dir + '/glove6b/glove.6B.%sd.txt'%(embedding_dim), encoding=\"utf-8\") as f:\n","    for line in f:\n","        values = line.split()\n","        word = values[0]\n","        coefs = np.asarray(values[1:], dtype='float32')\n","        embeddings_index[word] = coefs\n","    f.close()\n","print('Found %s word vectors.' % len(embeddings_index))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"0QsnpPrNfhLP","colab_type":"code","colab":{}},"source":["# Get dense vector for each of the 10000 words in out vocabulary\n","embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","for word, i in (tokenizer.word_index).items():\n","    #if i < max_words:\n","    embedding_vector = embeddings_index.get(word)\n","    if embedding_vector is not None:\n","        # Words not found in the embedding index will be all zeros\n","        embedding_matrix[i] = embedding_vector"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QJPjW_PkfhLQ","colab_type":"code","colab":{}},"source":["embedding_matrix.shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TxvdWF3rfhLT","colab_type":"text"},"source":["## Model"]},{"cell_type":"code","metadata":{"id":"hQjPy1trfhLT","colab_type":"code","colab":{}},"source":["# Extract features\n","inputs1 = tf.keras.Input(shape=(2048,))\n","fe1 = tf.keras.layers.Dropout(0.5)(inputs1)\n","fe2 = tf.keras.layers.Dense(256, activation='relu')(fe1)\n","# Sequence model\n","inputs2 = tf.keras.Input(shape=(max_length,))\n","se1 = tf.keras.layers.Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n","se2 = tf.keras.layers.Dropout(0.5)(se1)\n","se3 = tf.keras.layers.LSTM(256)(se2)\n","# Decoder model\n","decoder1 = tf.keras.layers.Add()([fe2, se3])\n","decoder2 = tf.keras.layers.Dense(256, activation='relu')(decoder1)\n","outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder2)\n","# Final model\n","model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-p0_zneZfhLV","colab_type":"code","colab":{}},"source":["model.layers[2].set_weights([embedding_matrix])\n","model.layers[2].trainable = False"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"NV9ZBIjFfhLY","colab_type":"code","colab":{}},"source":["model.compile(loss='categorical_crossentropy', optimizer='adam')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"WX0t4CqDAFKC","colab_type":"code","colab":{}},"source":["tf.keras.utils.plot_model(model, show_shapes=True, show_layer_names=True)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qVdRm5DBAqWg","colab_type":"code","colab":{}},"source":["model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dkROK4u4fhLd","colab_type":"text"},"source":["### Training"]},{"cell_type":"code","metadata":{"id":"SZvjq4PYfhLd","colab_type":"code","colab":{}},"source":["def generate_sequences(tokenizer, max_length, caption, image_feature):\n","    Ximages, XSeq, y = list(), list(),list()\n","    vocab_size = len(tokenizer.word_index)\n","    seq = tokenizer.texts_to_sequences([caption])[0]\n","    # split one sequence into multiple X,y pairs\n","    for i in range(1, len(seq)):\n","        # select\n","        in_seq, out_seq = seq[:i], seq[i]\n","        # pad input sequence\n","        in_seq = tf.keras.preprocessing.sequence.pad_sequences([in_seq], maxlen=max_length, padding='post')[0]\n","        # encode output sequence\n","        out_seq = tf.keras.utils.to_categorical([out_seq], num_classes=vocab_size)[0]\n","\n","        image_feature = np.squeeze(image_feature)\n","        Ximages.append(image_feature)\n","        XSeq.append(in_seq)\n","        y.append(out_seq)\n","    Ximages, XSeq, y = np.array(Ximages), np.array(XSeq), np.array(y)\n","    return [Ximages, XSeq, y]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"FtfdLOcdfhLh","colab_type":"code","colab":{}},"source":["def data_generator(tokenizer, max_length, captions, images_features, batch_size):\n","    # loop for ever over images\n","    n = 0\n","    while True:\n","        for i in range(len(captions)):\n","            in_img_vector = []\n","            in_seq_vector = []\n","            out_word_vector = []\n","            # load an image from file\n","            image_feature = images_features[i]\n","            # create word sequences\n","            caption = captions[i]\n","            in_img, in_seq, out_word = generate_sequences(tokenizer, max_length, caption, image_feature)\n","            in_img_vector.append(in_img)\n","            in_seq_vector.append(in_seq)\n","            out_word_vector.append(out_word)\n","            n += 1\n","            if n == batch_size:\n","                n = 0\n","                yield [in_img, in_seq], out_word"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnsoCvd8fhLj","colab_type":"code","colab":{}},"source":["# generate a description for an image\n","def generate_desc(model, tokenizer, photo, max_length):\n","    # seed the generation process\n","    in_text = 'start_sentence'\n","    for i in range(max_length):\n","        sequence = tokenizer.texts_to_sequences([in_text])[0]\n","        sequence = tf.keras.preprocessing.sequence.pad_sequences([sequence], maxlen=max_length, padding='post')\n","        \n","        photo = photo.reshape(2048,1).T\n","        yhat = model.predict([photo,sequence], verbose=0)\n","        yhat = np.argmax(yhat)\n","        word = tokenizer.index_word[yhat]\n","        if word is None:\n","            break\n","        in_text += ' ' + word\n","        if word == 'end_sentence':\n","            break\n","\n","    in_text = re.sub(r'(start|end)_sentence', '', in_text).strip()\n","    return in_text"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BFdxaO4BfhLk","colab_type":"code","colab":{}},"source":["# Store defautl weights\n","epoch_idx = 0\n","model_directory = './model'\n","if not os.path.exists(model_directory):\n","    os.makedirs(model_directory)\n","model.save_weights(model_directory + '/model.h5')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Hkuf8_AYfhLm","colab_type":"code","colab":{}},"source":["# After trying out different batch sizes, 1000 seems to provide reasonable results\n","training_record = {}\n","epochs = 20\n","batch_size = 1000"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"bUNvLJ_GfhLn","colab_type":"code","colab":{}},"source":["# Reset model with default weights before training\n","model.load_weights(model_directory + '/model.h5')\n","\n","steps = len(captions)//batch_size\n","training_record[batch_size] = []\n","epoch_idx = 0\n","\n","while epoch_idx < epochs:\n","    generator = data_generator(tokenizer, max_length, captions, images_features, batch_size)\n","    model.fit_generator(generator, steps_per_epoch=steps, verbose=1)\n","    model.save_weights(model_directory + '/model_epoch%s_bs%s.h5'%(epoch_idx, batch_size))\n","\n","    epoch_idx += 1\n","\n","    distinct_desc = []\n","    temp_desc = ''\n","    for j in range(20):\n","        img_desc = generate_desc(model, tokenizer, images_features[j], max_length)\n","        distinct_desc.append(img_desc)\n","        temp_desc += '' + img_desc\n","\n","    training_record[batch_size].append(distinct_desc)\n","    print(\"Model %s generated %s distinct captions with %s distinct words.\"%(epoch_idx, \n","                                                                             len(list(set(distinct_desc))),\n","                                                                             len(list(set(temp_desc.split(' '))))))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lJQzcEjGfhLp","colab_type":"code","colab":{}},"source":["# Train images\n","print(\"Training Performance with a batch size of %s and %s epochs.\\n\"%(batch_size, epochs - 1))\n","\n","model.load_weights(model_directory + '/model_epoch%s_bs%s.h5'%(epochs - 1, batch_size))\n","for j in np.random.choice(range(len(test_captions)), 5):\n","    # Compare captions\n","    img_desc = generate_desc(model, tokenizer, test_images_features[j], max_length)\n","    actual_caption = test_captions[j]\n","    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption)\n","\n","    print(\"Predicted Caption: %s \\nActual Caption: %s\"%(img_desc, actual_caption))\n","    # Show image\n","    I = io.imread(test_images[j])\n","    plt.imshow(I); plt.axis('off'); plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WInRtH_JfhLq","colab_type":"text"},"source":["### Tuning"]},{"cell_type":"code","metadata":{"id":"-zJspnx5fhLr","colab_type":"code","colab":{}},"source":["training_record = {}\n","epochs = 20"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mzcWw2AzgMPM","colab_type":"code","colab":{}},"source":["# TODO: https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2cChSSb4fhLt","colab_type":"code","colab":{}},"source":["# Trying out different batch sizes\n","for batch_size in [50, 100, 500, 1000]:\n","    model.load_weights(model_directory + '/model.h5')\n","    steps = len(captions)//batch_size\n","    print(batch_size)\n","    training_record[batch_size] = []\n","    epoch_idx = 0\n","    while epoch_idx < epochs:\n","        generator = data_generator(tokenizer, max_length, captions, images_features, batch_size)\n","        model.fit_generator(generator, steps_per_epoch=steps, verbose=1)\n","        model.save_weights(model_directory + '/model_epoch' + str(epoch_idx) + \"_bs\" + str(batch_size) +'.h5')\n","        epoch_idx += 1\n","\n","        distinct_desc = []\n","        temp_desc = ''\n","        for j in range(20):\n","            img_desc = generate_desc(model, tokenizer, images_features[j], max_length)\n","            distinct_desc.append(img_desc)\n","            temp_desc += '' + img_desc\n","        \n","        training_record[batch_size].append(distinct_desc)\n","        print(\"Model %s generated %s distinct captions with %s distinct words.\"%(epoch_idx, \n","                                                                                 len(list(set(distinct_desc))),\n","                                                                                 len(list(set(temp_desc.split(' '))))))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"diAcxjO1fhLu","colab_type":"text"},"source":["#### Test on Training Data"]},{"cell_type":"markdown","metadata":{"id":"lF8EEQS7fhLu","colab_type":"text"},"source":["Choose a random image"]},{"cell_type":"code","metadata":{"id":"E8jE4yGsfhLv","colab_type":"code","colab":{}},"source":["np.random.seed(0)\n","rand_train_image = np.random.choice(range(len(captions)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"vT84EevvfhLz","colab_type":"code","colab":{}},"source":["I = io.imread(images[rand_train_image])\n","plt.imshow(I); plt.axis('off'); plt.show()\n","\n","actual_caption = captions[rand_train_image]\n","actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption).strip()\n","print(\"The actual caption is:\\n%s \\n\\nThe predicted captions are:\"%(actual_caption))\n","    \n","for bs in [50, 100, 500, 1000]:\n","    model.load_weights(model_directory + '/model_epoch%s_bs%s.h5'%(epochs - 1, bs))\n","    # Compare captions\n","    img_desc = generate_desc(model, tokenizer, images_features[rand_train_image], max_length)\n","\n","    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption)\n","    print(\"Batch size = %s: %s\"%(bs, img_desc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s5BvNKC3fhL0","colab_type":"text"},"source":["#### Test on Validation Data"]},{"cell_type":"code","metadata":{"id":"cRE0Vp3LfhL1","colab_type":"code","colab":{}},"source":["rand_test_image = np.random.choice(range(len(test_captions)))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JwxqDNxyfhL2","colab_type":"code","colab":{}},"source":["I = io.imread(images[rand_test_image])\n","plt.imshow(I); plt.axis('off'); plt.show()\n","\n","actual_caption = captions[rand_test_image]\n","actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption).strip()\n","print(\"The actual caption is:\\n%s \\n\\nThe predicted captions are:\"%(actual_caption))\n","    \n","for bs in [50, 100, 500, 1000]:\n","    model.load_weights(model_directory + '/model_epoch%s_bs%s.h5'%(epochs - 1, bs))\n","    # Compare captions\n","    img_desc = generate_desc(model, tokenizer, images_features[rand_test_image], max_length)\n","\n","    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption)\n","    print(\"Batch size = %s: %s\"%(bs, img_desc))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sjzB24t2fhL4","colab_type":"text"},"source":["## Discussion"]},{"cell_type":"markdown","metadata":{"id":"v2_6K92gIxmZ","colab_type":"text"},"source":["### Results"]},{"cell_type":"markdown","metadata":{"id":"5vWSKkWrUXmX","colab_type":"text"},"source":["The model is able to provide reasonable captions given the images supplied. However, there remain potential for improvements. In the cases where the model is not able to provide sensible captions, it is apparent from the images and the captions that the model has generalized some patterns in the images that its not able to make an educated guess. For instance, when applied to a image of a man doing a ski jump"]},{"cell_type":"markdown","metadata":{"id":"MXMc3w7_I7e0","colab_type":"text"},"source":["### What Worked"]},{"cell_type":"markdown","metadata":{"id":"F-P2Qu6G6MDa","colab_type":"text"},"source":["### What did not and why not"]},{"cell_type":"markdown","metadata":{"id":"YUKSxjc66OwJ","colab_type":"text"},"source":["#### Word Distributions"]},{"cell_type":"markdown","metadata":{"id":"fzozPmTa6Vm8","colab_type":"text"},"source":["The model has a slightly biased distribution of words. "]},{"cell_type":"code","metadata":{"id":"CFrE1tqh6J9e","colab_type":"code","colab":{}},"source":["# Understand vocab distribution\n","word_list = []\n","for caption in captions:\n","    seq = caption.split(\" \")\n","    word_list += seq\n","\n","lemmatizer = WordNetLemmatizer()\n","word_list = [lemmatizer.lemmatize(word.lower()) for word in word_list \n","                if word and \n","                    word.lower() not in nltk.corpus.stopwords.words('english') and\n","                    \"_\" not in word]\n","\n","# Calculate frequency distribution\n","word_list_dist = nltk.FreqDist(word_list)\n","\n","# Output top 25 words\n","limit = 50\n","word_dict = dict()\n","for word, frequency in word_list_dist.most_common(limit):\n","    print(u'{};{}'.format(word, frequency))\n","    word_dict[word] = frequency"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9nSuQl02JBXe","colab_type":"text"},"source":["### Lessons Learned"]},{"cell_type":"code","metadata":{"id":"LzY6BPKlJCeG","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}