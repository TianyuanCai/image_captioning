{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ImageCaptioning_CaiTianyuan_Solution.ipynb",
   "version": "0.3.2",
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title": "Generation of Captions from Images",
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "238px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyjpQWV-HUT8",
    "colab_type": "text"
   },
   "source": [
    "CSCI S-89a Deep Learning, Summer 2019\n",
    "\n",
    "Tianyuan Cai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "j0DEAXPRIfdQ"
   },
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7wWA0QcgIomW"
   },
   "source": [
    "The goal of this project is to generate sensible captions for images. Image captioning has a large variety of applications. By understanding common objects in an image and transforming them into textual information, we can compress large photographic information into more compact data formats. The possibility of this type of transformation makes it easier to make inferences based on photographic data and gives rise to applications such as voice over technologies, recommender engine, etc.\n",
    "\n",
    "In this project, I use the Inception-V3 and GloVe implementations to transform images and words into feature vectors. I then pass these data through LSTM hidden layer and produce sensible captions using word-by-word predictions. The training and validation data come from Common Objects in Context (COCO) dataset produced by Microsoft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ztrAd6kjfhJ5"
   },
   "source": [
    "# Preparing Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7cU2qwpfhJ5"
   },
   "source": [
    "I used Anaconda with Python 3.7 run on an Ubuntu 18.04 system to train and test this model. An Nvidia Titan Xp GPU was used to train the model, and 32 GB of RAM was available during training. The final data take up around 20 GB. All data are downloaded and extracted within the Jupyter Notebook, so the submission only requires the Jupyter Notebook itself to run. It helps, however, to run the notebook in an empty project folder to ensure the required data can be unpacked smoothly in the folder. \n",
    "\n",
    "Despite the availability of relatively powerful graphic card, I still made use of methods such as the tqdm package, data generator class of Python, and fit generator function from Keras to accommodate systems that are less powerful. Batch size, number of training images and captions, sentence length, etc. can all be adjusted to accommodate a system of lower configuration. The notebook is tested on both remote Ubuntu server and Google Colab to make sure there is no error when running. Here are some system preparations needed before starting to run the Jupyter Notebook. \n",
    "\n",
    "* Make sure there are at least 40 GB of disk space and sufficient memory available before running the code. \n",
    "* Install TensorFlow GPU in Anaconda by running the following command. It will install the package as well as all dependencies, such as Cuda and cuDNN.\n",
    "conda install -c anaconda tensorflow-gpu\n",
    "* After installing TensorFlow, use pip to install the following packages. pycocotools is dependent on Cython. Therefore, Cython must be installed first. \n",
    "    - `pip install pillow numpy matplotlibpydotpylabscikit-image keras Cython pycocotools nltk seaborntqdm`\n",
    "* Data set used by this code will all be downloaded to the project folder. Change the line `data_dir = \"/home/tcai/Documents/nlp/final_project\"` to your own project folder. For instance, if you are using Google Colab to run this code, you can change this line to `data_dir = \"/content\"`. \n",
    "* Once the packages are installed and the data directory updated. Running the notebook does not require any additional setup. All the data will download into the project folder, and the model will be saved by epoch as it trains with appropriate names created for them.  \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "-Z5eMvabfhJ6",
    "outputId": "819f69d2-09b7-4aaa-dd0a-0b2cd5d49c88",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1568067672720,
     "user_tz": 240,
     "elapsed": 9006,
     "user": {
      "displayName": "Tianyuan Cai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAqZW7Wi5hxsABUsn0BxLIS49KSqJ0cmHS2UNfjxsQ=s64",
      "userId": "13228707440502595479"
     }
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Install packages\n",
    "!pip install --user pillow numpy scikit-image keras nltk seaborn\n",
    "!pip install --user Cython\n",
    "!pip install --user pycocotools\n",
    "\n",
    "# # If there's error in pycocotools installation, try installing directly from the repo\n",
    "# !pip3 install \"git+https://github.com/philferriere/cocoapi.git#egg=pycocotools&subdirectory=PythonAPI\""
   ],
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/site-packages (6.1.0)\r\nRequirement already satisfied: numpy in /usr/local/lib/python3.7/site-packages (1.17.1)\r\nRequirement already satisfied: scikit-image in /Users/tcai/Library/Python/3.7/lib/python/site-packages (0.15.0)\r\nRequirement already satisfied: keras in /usr/local/lib/python3.7/site-packages (2.2.5)\r\n",
      "Requirement already satisfied: nltk in /usr/local/lib/python3.7/site-packages (3.4.5)\r\nRequirement already satisfied: seaborn in /usr/local/lib/python3.7/site-packages (0.9.0)\r\nRequirement already satisfied: networkx>=2.0 in /Users/tcai/Library/Python/3.7/lib/python/site-packages (from scikit-image) (2.3)\r\n",
      "Requirement already satisfied: imageio>=2.0.1 in /Users/tcai/Library/Python/3.7/lib/python/site-packages (from scikit-image) (2.5.0)\r\nRequirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/site-packages (from scikit-image) (1.3.1)\r\nRequirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-image) (3.1.1)\r\nRequirement already satisfied: PyWavelets>=0.4.0 in /Users/tcai/Library/Python/3.7/lib/python/site-packages (from scikit-image) (1.0.3)\r\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/site-packages (from keras) (5.1.2)\r\nRequirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.1.0)\r\nRequirement already satisfied: h5py in /usr/local/lib/python3.7/site-packages (from keras) (2.9.0)\r\n",
      "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/site-packages (from keras) (1.12.0)\r\nRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/site-packages (from keras) (1.0.8)\r\nRequirement already satisfied: pandas>=0.15.2 in /usr/local/lib/python3.7/site-packages (from seaborn) (0.25.1)\r\nRequirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.7/site-packages (from networkx>=2.0->scikit-image) (4.4.0)\r\nRequirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.4.2)\r\nRequirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (2.8.0)\r\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (0.10.0)\r\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image) (1.1.0)\r\nRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas>=0.15.2->seaborn) (2019.2)\r\nRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image) (41.2.0)\r\n",
      "Requirement already satisfied: Cython in /Users/tcai/Library/Python/3.7/lib/python/site-packages (0.29.13)\r\n",
      "Collecting pycocotools\r\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/84/9a07b1095fd8555ba3f3d519517c8743c2554a245f9476e5e39869f948d2/pycocotools-2.0.0.tar.gz (1.5MB)\r\n",
      "\r\u001b[K     |▏                               | 10kB 214kB/s eta 0:00:07\r\u001b[K     |▍                               | 20kB 426kB/s eta 0:00:04\r\u001b[K     |▋                               | 30kB 628kB/s eta 0:00:03",
      "\r\u001b[K     |▉                               | 40kB 435kB/s eta 0:00:04\r\u001b[K     |█                               | 51kB 527kB/s eta 0:00:03\r\u001b[K     |█▎                              | 61kB 631kB/s eta 0:00:03\r\u001b[K     |█▌                              | 71kB 715kB/s eta 0:00:02",
      "\r\u001b[K     |█▊                              | 81kB 598kB/s eta 0:00:03\r\u001b[K     |██                              | 92kB 671kB/s eta 0:00:03\r\u001b[K     |██▏                             | 102kB 744kB/s eta 0:00:02\r\u001b[K     |██▍                             | 112kB 744kB/s eta 0:00:02\r\u001b[K     |██▋                             | 122kB 744kB/s eta 0:00:02\r\u001b[K     |██▉                             | 133kB 744kB/s eta 0:00:02\r\u001b[K     |███                             | 143kB 744kB/s eta 0:00:02\r\u001b[K     |███▎                            | 153kB 744kB/s eta 0:00:02\r\u001b[K     |███▌                            | 163kB 744kB/s eta 0:00:02\r\u001b[K     |███▊                            | 174kB 744kB/s eta 0:00:02\r\u001b[K     |████                            | 184kB 744kB/s eta 0:00:02\r\u001b[K     |████▏                           | 194kB 744kB/s eta 0:00:02\r\u001b[K     |████▍                           | 204kB 744kB/s eta 0:00:02\r\u001b[K     |████▋                           | 215kB 744kB/s eta 0:00:02",
      "\r\u001b[K     |████▉                           | 225kB 744kB/s eta 0:00:02\r\u001b[K     |█████                           | 235kB 744kB/s eta 0:00:02\r\u001b[K     |█████▎                          | 245kB 744kB/s eta 0:00:02\r\u001b[K     |█████▌                          | 256kB 744kB/s eta 0:00:02\r\u001b[K     |█████▊                          | 266kB 744kB/s eta 0:00:02\r\u001b[K     |██████                          | 276kB 744kB/s eta 0:00:02\r\u001b[K     |██████▏                         | 286kB 744kB/s eta 0:00:02\r\u001b[K     |██████▍                         | 296kB 744kB/s eta 0:00:02\r\u001b[K     |██████▋                         | 307kB 744kB/s eta 0:00:02\r\u001b[K     |██████▉                         | 317kB 744kB/s eta 0:00:02\r\u001b[K     |███████                         | 327kB 744kB/s eta 0:00:02\r\u001b[K     |███████▎                        | 337kB 744kB/s eta 0:00:02\r\u001b[K     |███████▌                        | 348kB 744kB/s eta 0:00:02\r\u001b[K     |███████▊                        | 358kB 744kB/s eta 0:00:02\r\u001b[K     |████████                        | 368kB 744kB/s eta 0:00:02\r\u001b[K     |████████▏                       | 378kB 744kB/s eta 0:00:02\r\u001b[K     |████████▍                       | 389kB 744kB/s eta 0:00:02\r\u001b[K     |████████▋                       | 399kB 744kB/s eta 0:00:02\r\u001b[K     |████████▉                       | 409kB 744kB/s eta 0:00:02\r\u001b[K     |█████████                       | 419kB 744kB/s eta 0:00:02",
      "\r\u001b[K     |█████████▎                      | 430kB 744kB/s eta 0:00:02\r\u001b[K     |█████████▌                      | 440kB 744kB/s eta 0:00:02\r\u001b[K     |█████████▊                      | 450kB 744kB/s eta 0:00:02\r\u001b[K     |██████████                      | 460kB 744kB/s eta 0:00:02\r\u001b[K     |██████████▏                     | 471kB 744kB/s eta 0:00:02\r\u001b[K     |██████████▍                     | 481kB 744kB/s eta 0:00:02\r\u001b[K     |██████████▋                     | 491kB 744kB/s eta 0:00:02\r\u001b[K     |██████████▉                     | 501kB 744kB/s eta 0:00:02\r\u001b[K     |███████████                     | 512kB 744kB/s eta 0:00:02\r\u001b[K     |███████████▎                    | 522kB 744kB/s eta 0:00:02\r\u001b[K     |███████████▌                    | 532kB 744kB/s eta 0:00:02\r\u001b[K     |███████████▊                    | 542kB 744kB/s eta 0:00:02\r\u001b[K     |████████████                    | 552kB 744kB/s eta 0:00:02\r\u001b[K     |████████████▏                   | 563kB 744kB/s eta 0:00:02\r\u001b[K     |████████████▍                   | 573kB 744kB/s eta 0:00:02\r\u001b[K     |████████████▋                   | 583kB 744kB/s eta 0:00:02\r\u001b[K     |████████████▉                   | 593kB 744kB/s eta 0:00:02\r\u001b[K     |█████████████                   | 604kB 744kB/s eta 0:00:02\r\u001b[K     |█████████████▎                  | 614kB 744kB/s eta 0:00:02\r\u001b[K     |█████████████▌                  | 624kB 744kB/s eta 0:00:02\r\u001b[K     |█████████████▊                  | 634kB 744kB/s eta 0:00:02\r\u001b[K     |██████████████                  | 645kB 744kB/s eta 0:00:02\r\u001b[K     |██████████████▏                 | 655kB 744kB/s eta 0:00:02\r\u001b[K     |██████████████▍                 | 665kB 744kB/s eta 0:00:02\r\u001b[K     |██████████████▋                 | 675kB 744kB/s eta 0:00:02\r\u001b[K     |██████████████▉                 | 686kB 744kB/s eta 0:00:02",
      "\r\u001b[K     |███████████████                 | 696kB 744kB/s eta 0:00:02\r\u001b[K     |███████████████▎                | 706kB 744kB/s eta 0:00:02\r\u001b[K     |███████████████▌                | 716kB 744kB/s eta 0:00:02\r\u001b[K     |███████████████▊                | 727kB 744kB/s eta 0:00:02\r\u001b[K     |████████████████                | 737kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 747kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 757kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 768kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 778kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████               | 788kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 798kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 808kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 819kB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████              | 829kB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 839kB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 849kB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 860kB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 870kB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████             | 880kB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 890kB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 901kB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 911kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████            | 921kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 931kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 942kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 952kB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 962kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 972kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 983kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 993kB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.0MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.0MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.0MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.0MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1MB 744kB/s eta 0:00:01",
      "\r\u001b[K     |███████████████████████         | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████▌        | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████▎    | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████▊    | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.5MB 744kB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.5MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 744kB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 744kB/s \r\n",
      "\u001b[?25hBuilding wheels for collected packages: pycocotools\r\n",
      "  Building wheel for pycocotools (setup.py) ... \u001b[?25l-"
     ],
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "bfK1X_n31B7j",
    "outputId": "e2fc050c-8dba-47f0-802d-17c1a37a5372",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1568067682413,
     "user_tz": 240,
     "elapsed": 7417,
     "user": {
      "displayName": "Tianyuan Cai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAqZW7Wi5hxsABUsn0BxLIS49KSqJ0cmHS2UNfjxsQ=s64",
      "userId": "13228707440502595479"
     }
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Download NLTK resources\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "-dSz2wp1fhJ-",
    "outputId": "0fbe8a68-feb7-4014-8ad2-28879103faf9",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1568067685629,
     "user_tz": 240,
     "elapsed": 6603,
     "user": {
      "displayName": "Tianyuan Cai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAqZW7Wi5hxsABUsn0BxLIS49KSqJ0cmHS2UNfjxsQ=s64",
      "userId": "13228707440502595479"
     }
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Import packages and configure\n",
    "import os\n",
    "import pickle\n",
    "from time import time\n",
    "import json\n",
    "import re\n",
    "\n",
    "import sklearn\n",
    "import keras\n",
    "from PIL import Image\n",
    "import skimage.io as io\n",
    "import seaborn as sns\n",
    "\n",
    "import keras.applications.imagenet_utils\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pylab\n",
    "pylab.rcParams['figure.figsize'] = (4, 6)\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "\n",
    "# The kernal occasionally dies on Google Colab when installing and importing pycocotools at the same time\n",
    "# It's good to install the pycocotools first, and run the import code in a separate session\n",
    "from pycocotools.coco import COCO"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HcuNaWygfhKA",
    "outputId": "3c20dc68-18f2-4972-ce1b-119038ebcc4d",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1568067690067,
     "user_tz": 240,
     "elapsed": 2380,
     "user": {
      "displayName": "Tianyuan Cai",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mAqZW7Wi5hxsABUsn0BxLIS49KSqJ0cmHS2UNfjxsQ=s64",
      "userId": "13228707440502595479"
     }
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Test if running on GPU\n",
    "keras.backend.tensorflow_backend._get_available_gpus()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vxPXfeSwEZIo"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N6DVeX2RoyDo"
   },
   "source": [
    "## Data Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TrwlBEz-E6NU"
   },
   "source": [
    "Three data sets are downloaded and unpacked as the code runs:\n",
    "\n",
    "* Training image data set from Common Objects in Context (COCO) data set  produced by Microsoft. Due to the size of this data set, I obtain a subset of the downloaded data set and split it into train and validation set.\n",
    "\n",
    "* Captions of the training images from COCO . This data set contain captions that correspond to the images in the training images data set.\n",
    "\n",
    "The COCO dataset provides a set of images of common objects in their natural context. The images are collected from a variety of sources, and the corresponding caption data provide multiple short captions to each image. The captions describe the objects in and the context of the image. Furthermore, the COCO API can be used to categorized by their super-categories, such as animal, furniture, sports, etc. for easy access. Here is an example of an image from the COCO data together with its corresponding captions.\n",
    "\n",
    "See [Coco API](https://github.com/cocodataset/cocoapi/blob/master/PythonAPI/pycocoDemo.ipynb) for package usage. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjtrF1VafhKF"
   },
   "source": [
    "We start by extracting image and caption (annotation) data from COCO website. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yCd4W5MKHUUM",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Get current working directory\n",
    "!pwd"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HixAVIUzygZ-",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Specify data directory and the COCO training file to be used\n",
    "# Update the following directory with the new working directory found by the previous command\n",
    "data_dir = \"/home/tcai/Documents/nlp/final_project\"\n",
    "data_type = \"train2017\"\n",
    "data_zipfile = '%s.zip' % (data_type)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p98z-Eyb1B7r"
   },
   "source": [
    "Run the following command only once to obtain and extract files. If the files already exist, remove them to ensure the `get_file` commands can run smoothly."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ytha400S00x7",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Download annotation, image, and glove\n",
    "annotation_zip = tf.keras.utils.get_file('captions.zip',\n",
    "                                         cache_subdir=os.path.abspath('.'),\n",
    "                                         origin = 'http://images.cocodataset.org/annotations/annotations_trainval2017.zip',\n",
    "                                         extract = True)\n",
    "image_zip = tf.keras.utils.get_file(data_zipfile,\n",
    "                                    cache_subdir=os.path.abspath('.'),\n",
    "                                    origin = 'http://images.cocodataset.org/zips/%s'%(data_zipfile),\n",
    "                                    extract = True)\n",
    "glove6b_zip = tf.keras.utils.get_file('glove.6B.zip',\n",
    "                                    cache_subdir=os.path.abspath('./glove6b'),\n",
    "                                    origin = 'http://nlp.stanford.edu/data/glove.6B.zip',\n",
    "                                    extract = True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "XabFXwZ701EI",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Update file directory objects\n",
    "annotation_file = data_dir + '/annotations/captions_%s.json' % (data_type)\n",
    "image_dir = data_dir + '/%s/' % (data_type)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ovCgXdJ-fhKH"
   },
   "source": [
    "Coco provides images that are categorized into a variety of categories. The categories listed below can be used to index images in the data set."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "jchnZh2iFt1F",
    "outputId": "0afdb61f-71b7-40a1-cd3f-f2b146381efb",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Initialize COCO api to show sample images and match images with captions\n",
    "coco_caps = COCO(annotation_file)\n",
    "\n",
    "# Obtain categories\n",
    "annFile = '{}/annotations/instances_{}.json'.format(data_dir, data_type)\n",
    "coco = COCO(annFile)\n",
    "\n",
    "cats = coco.loadCats(coco.getCatIds())\n",
    "nms = [cat['name'] for cat in cats]\n",
    "print('\\nCOCO Categories: \\n{}\\n'.format(' '.join(nms)))\n",
    "\n",
    "nms = set([cat['supercategory'] for cat in cats])\n",
    "print('COCO Supercategories: \\n{}'.format(' '.join(nms)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tdCSzSPZ1B7x"
   },
   "source": [
    "COCO API has a handy feature that allows the user to see images by custom categories. For instance, when inputing the feature `['dog','person', 'ball']`, here is what the model shows."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "mDJCZrLuFg1A",
    "outputId": "c606ca16-1843-4538-e5a4-be9d41ecd5c8",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 514
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Show sample data set by choosing categories\n",
    "temp_cat = ['dog', 'person', 'ball']\n",
    "\n",
    "catIds = coco.getCatIds(catNms=temp_cat)\n",
    "imgIds = coco.getImgIds(catIds=catIds)\n",
    "\n",
    "# Identify relevant images\n",
    "if len(imgIds) > 0:\n",
    "    imgIds = coco.getImgIds(imgIds=imgIds[-1])  # Pick the last image\n",
    "    print(\"The index of the chosen image is %s.\\n\" % (str(imgIds[0])))\n",
    "else:\n",
    "    print(\"No matched images found.\")\n",
    "\n",
    "# Load and display captions\n",
    "annIds = coco_caps.getAnnIds(imgIds)\n",
    "anns = coco_caps.loadAnns(annIds)\n",
    "print(\"The corresponding captions are:\")\n",
    "coco_caps.showAnns(anns)\n",
    "\n",
    "# Show image\n",
    "img = coco.loadImgs(imgIds)[0]\n",
    "I = io.imread('%s/%s/%s' % (data_dir, data_type, img['file_name']))\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mBhzPXy2VJ6S"
   },
   "source": [
    "The concise captions provide several layers of information including the objects, their relationships, and the context or the background of the image, each with some variety. For instance, note that in the image above, the person is said to be “a man”, “a guy”, and “the man”; The dog is said to be “Dalmatian dog”, “dalmatian”, and “a dog”. Furthermore, the captions identify the relationship between the objects shown – the man is said to have a leash on his dog or walking the dog; the ownership of the dog is sometimes identified as well. Furthermore, relationship between the objects and the context is also accurately reflected – the man and the dog are walking “in front of a firetruck”. \n",
    "\n",
    "However, the captions have their idiosyncrasies beyond semantic meanings. One caption has the first letter in lowercase, and others do not. One caption capitalized the first letter in “Dalmatian” while others do not. When processing the caption data, I transformed all words and removed special characters to ensure the captions are consistent. Similarly, the images in the data set also have inconsistent qualities. Therefore, additional preprocessing is performed on the images, such as converting images to 299 by 299 dimension before encoding them with the Inception V3 model.\n",
    "\n",
    "When preparing the dataset, the captions, images paths, and encoded image features are all needed in order to effectively implemented the training. Rather than using train_test_split function, I choose to shuffle the data and subset to the desired number of train and test samples. Related data of each category share the same index throughout the analysis.\n",
    "Again, note that the data set used by this code will all be created in the project folder. Look for the line `data_dir = “/home/tcai/Documents/nlp/final_project”` and change this directory into yours. Once the data set is downloaded and the images shuffled, I moved on to create image and text embeddings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pjj7NxeAfhKP"
   },
   "source": [
    "## Train-test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sqM4cmTAfhKQ"
   },
   "source": [
    "Captions, images paths, and encoded image features are needed in order to effectively implemented the training. Rather than using `train_test_split` function, I choose to shuffle the data and subset to obtain a 80/20 split of train and validation data."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "MNsn4BSmfhKR",
    "outputId": "2ecb0269-f44b-4f06-b397-7dd663dc211e",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Specify total number of samples to be used for training and testing.\n",
    "total_examples = 100000\n",
    "train_examples = int(0.8 * total_examples)\n",
    "\n",
    "# Subset training and testing images\n",
    "with open(annotation_file, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "all_img_path_vector = {}\n",
    "all_img_name_vector = list(set([x['image_id'] for x in annotations['annotations']]))\n",
    "print(\n",
    "    \"Choosing %s training images and %s validation images from a total of %s images\"\n",
    "    % (train_examples, total_examples - train_examples, len(all_img_name_vector)))\n",
    "\n",
    "for img_id in all_img_name_vector:\n",
    "    img = coco.loadImgs(img_id)[0]\n",
    "    image_file_path = '%s/%s/%s' % (data_dir, data_type, img['file_name'])\n",
    "    all_img_path_vector[img_id] = image_file_path\n",
    "\n",
    "# Shuffle and obtain subset\n",
    "all_img_name_vector = sklearn.utils.shuffle(all_img_name_vector, random_state=0)\n",
    "all_img_name_vector = all_img_name_vector[:total_examples]\n",
    "\n",
    "# Obtain train and test set\n",
    "train_img_name_vector = all_img_name_vector[:train_examples]  # train\n",
    "test_img_name_vector = all_img_name_vector[train_examples:]  # test"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NAMRwe9NfhKT"
   },
   "source": [
    "# Image Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "O1-c1nYdfhKU"
   },
   "source": [
    "I use the Inception V3 model as an image encoder by removing the fully connected layers in the end. The lower dimensional representation of the images reduces the amount of data needed by the model to perform prediction of the captions. In addition, since the Inception V3 model is built to minimize the loss function for accurate object recognition, using the model as an encoder helps minimize this loss function for the training image data I used. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "fwvaNvTjfhKV",
    "outputId": "9484e652-ba09-42b2-8b92-4a0622421dc7",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Create the inception v3 model\n",
    "# take out the fully connected layers at the end to have it output image embeddings\n",
    "image_model = keras.applications.InceptionV3(weights='imagenet')\n",
    "feature_model = keras.models.Model(image_model.input,\n",
    "                                   image_model.layers[-2].output)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Exg9Vg6yfhKa"
   },
   "source": [
    "Define encoding function that:\n",
    "\n",
    "* Transform the image into consistent sizes\n",
    "\n",
    "* Convert the images to array\n",
    "\n",
    "* Expand dimensions\n",
    "\n",
    "* Pre-process the input\n",
    "\n",
    "* Reshape images"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Xc5X8zZNfhKc",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Encoding function for feature extraction\n",
    "def encode(image_path):\n",
    "    # Preprocess images\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = keras.applications.inception_v3.preprocess_input(img_array)\n",
    "    \n",
    "    # Produce image embeddings\n",
    "    fea_vec = feature_model.predict(img_array)\n",
    "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1])\n",
    "    return (fea_vec)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CJxHRKjtVX0h"
   },
   "source": [
    "Define a similar function but for showcasing Inception V3's prediction ability."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "l73t3nXAfhKh",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Similar function for testing how inception v3 model works\n",
    "def inceptionv3_predict(image_path, image_model):\n",
    "    # Preprocess images\n",
    "    img = tf.keras.preprocessing.image.load_img(image_path, target_size=(299, 299))\n",
    "    img_array = tf.keras.preprocessing.image.img_to_array(img)\n",
    "    img_array = np.expand_dims(img_array, axis=0)\n",
    "    img_array = keras.applications.inception_v3.preprocess_input(img_array)\n",
    "    preds = image_model.predict(img_array)\n",
    "    P = keras.applications.imagenet_utils.decode_predictions(preds)\n",
    "\n",
    "    # Show prediction result\n",
    "    for (i, (imagenetID, label, prob)) in enumerate(P[0]):\n",
    "        print(\"{}. {}: {:.2f}%\".format(i + 1, label, prob * 100))\n",
    "\n",
    "    # Show image\n",
    "    I = io.imread(image_path)\n",
    "    plt.imshow(I)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cSQ0phPGmetf"
   },
   "source": [
    "Below is an example of the prediction produced by the original Inception V3 model. The model outputs a score for each observed item, reflecting its confidence in the prediction."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "R72kjxv5mdp7",
    "outputId": "4ed520f4-2fbf-4f4e-99af-c29a00a076a5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 320
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Randomly choose images to be predicted by Inception V3\n",
    "random_img_id = np.random.choice(all_img_name_vector)\n",
    "random_img_path = all_img_path_vector[random_img_id]\n",
    "inceptionv3_predict(random_img_path, image_model)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LGKT0WXZxE6A"
   },
   "source": [
    "To encode the image using Inception V3, I first transform images into 299 by 299 dimension, convert them to arrays, and then use the preprocessing function in Keras to preprocess the images. I then transfer the weight learned by Inception V3 on ImageNet data onto predicting the training data set. The output of the predict function is the encoded image features, and I store the output in a pickle file, indexed by image IDs so that I can use them for future uses. The 100,000 images in the training and validation data set took around 2,100 seconds to encode. The composition of the final data set is as follows:\n",
    "\n",
    "* Training: 80,000 distinct images, 400204 captions.\n",
    "* Validation: 20,000 distinct images, 100057 captions.\n",
    "\n",
    "Note that because each image is paired with multiple captions, the number of captions is larger than that of the images. When training images, I choose distinct images to ensure the same image is not encoded multiple times. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "be-CQouPfhKn",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Encode all the train images. Run once and store features in a pickle file\n",
    "start = time()\n",
    "train_features = {}\n",
    "for img_id in all_img_name_vector:\n",
    "    train_features[img_id] = encode(all_img_path_vector[img_id])\n",
    "\n",
    "# Took around 2100 seconds.\n",
    "print(\"Time taken in seconds =\", time() - start)\n",
    "\n",
    "# Pickle the features\n",
    "with open(data_dir + \"/encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
    "    pickle.dump(train_features, encoded_pickle)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "lZqXI7CufhKq",
    "outputId": "9467807c-ae14-4c9d-901a-2561cc217bda",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Load image features from the pickle file\n",
    "train_features = pickle.load(open(data_dir + \"/encoded_train_images.pkl\", \"rb\"))\n",
    "print('%d photos in total are encoded. These include both training and testing image set.' \n",
    "      % len(train_features))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SG8k_qhAfhKt"
   },
   "source": [
    "For each captions in the training and testing data set, I append images back onto the captions so that all captions and features are included."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ofnkV1n1fhKu",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Create the train image data set\n",
    "captions = []\n",
    "images = []\n",
    "images_features = []\n",
    "\n",
    "for img_id in train_img_name_vector:\n",
    "    img_path = all_img_path_vector[img_id]\n",
    "    img_feature = train_features[img_id]\n",
    "    img_captions = coco_caps.loadAnns(coco_caps.getAnnIds(img_id))\n",
    "\n",
    "    for caption in [x['caption'] for x in img_captions]:\n",
    "        captions.append('start_sentence ' + caption + ' end_sentence')\n",
    "        images.append(img_path)\n",
    "        images_features.append(img_feature)\n",
    "\n",
    "captions, images, images_features = sklearn.utils.shuffle(captions,\n",
    "                                                          images,\n",
    "                                                          images_features,\n",
    "                                                          random_state=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "J16ph71hfhKv",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Create the validation image data set\n",
    "test_captions = []\n",
    "test_images = []\n",
    "test_images_features = []\n",
    "\n",
    "for img_id in test_img_name_vector:\n",
    "    img_path = all_img_path_vector[img_id]\n",
    "    img_feature = train_features[img_id]\n",
    "    img_captions = coco_caps.loadAnns(coco_caps.getAnnIds(img_id))\n",
    "\n",
    "    for caption in [x['caption'] for x in img_captions]:\n",
    "        test_captions.append('start_sentence ' + caption + ' end_sentence')\n",
    "        test_images.append(img_path)\n",
    "        test_images_features.append(img_feature)\n",
    "\n",
    "test_captions, test_images, test_images_features = sklearn.utils.shuffle(\n",
    "    test_captions, test_images, test_images_features, random_state=0)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "NIL2J-xafhKx",
    "outputId": "3c727513-00a2-4249-be09-5e91f8bc2c8c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "print('Training: %s distinct images, %s captions,' %\n",
    "      (len(list(set(images))), len(captions)))\n",
    "print('Validation: %s distinct images, %s captions.' %\n",
    "      (len(list(set(test_images))), len(test_captions)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KTahEk0CwPf4"
   },
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yzcg4nU21B8K"
   },
   "source": [
    "Before using the captions in the training and prediction, the data are first processed to remove special characters and tokenized. Later on, as the sentence is separated by words and supplied to the model word-by-word as training and validation, the sentence is then padded to the maximum length. In addition, sentence starting tag “start_sentence” and ending tag “end_sentence” are added to the tokenizer and the training captions to help identify the end of a caption.\n",
    "\n",
    "After removing special characters and tokenize the words, the lengths of the sentences have the following distribution. I limit the maximum sentence length to 100, and the maximum number of words to 6,000. When setting the maximum sentence length, I referred to the sentence length distribution as follows. The value on the x-axis is the number of words in a sentence, and the values on the y-axis is the proportion. The overall area covered by the following histogram equals one. The line characterizes the kernel density estimation. Most of the captions are less than 100 words. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "8gzJIDNXwPC0",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Find the maximum length of any caption in our dataset\n",
    "def calc_max_length(tensor):\n",
    "    return max(len(t) for t in tensor)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "7MDWQEdwwl26",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "number_of_words = 6000\n",
    "\n",
    "# Choose the top words from the vocabulary\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer(\n",
    "    num_words=number_of_words,\n",
    "    oov_token=\"<unk>\",\n",
    "    filters='!\"#$%&()*+.,-/:;=?@[\\]^`{|}~ ')\n",
    "captions = [caption.lower() for caption in captions]\n",
    "tokenizer.fit_on_texts(captions)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "lDsqxthy5u9V",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Index the padding values\n",
    "tokenizer.word_index['<pad>'] = 0\n",
    "tokenizer.index_word[0] = '<pad>'"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "8hEjBeBe32IJ",
    "outputId": "26215a06-1ebf-4c0a-ae45-eb57b193d44c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 407
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Understand the distribution of sentence lengths\n",
    "sentence_lengths = [len(caption) for caption in captions]\n",
    "plt.title('Distribution of Caption Lengths')\n",
    "sns.distplot(sentence_lengths)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeoLnXVQqh0p"
   },
   "source": [
    "After preprocessing, I use Global Vectors for Word Representation (GloVe) to obtain embedding vectors for words in the captions. The position of a word within the vector space is learned based on the words that surround the word in the training data, in this case, on the Wikipedia 2014 and Gigaword 5 data (glove.6B.300d.txt). And I supplied the vector weights to be used in the embedding layer of the model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "dNS6tItWfhLJ",
    "outputId": "6b1ec0e8-55e3-4f40-85da-196afe279c11",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Calculates sentence and vocab lengths\n",
    "# Manually set the maximum sentence length after observing the distribution\n",
    "max_length = 100  \n",
    "vocab_size = len(tokenizer.index_word)\n",
    "print(\"Original max sentence length is %s; I set it to %s.\" %\n",
    "      (calc_max_length(captions), max_length))\n",
    "print(\"The vocabulary size is: %s\" % (vocab_size))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gnRSO1fZ1B8R"
   },
   "source": [
    "The `glove.6B.300d.txt` file obtaiend at the beginning of the notebook is used here. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "GqAzqESLfhLL",
    "outputId": "90526bd5-0d89-4ff3-ca5d-294517dd0a5a",
    "scrolled": true,
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "embedding_dim = 300\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(data_dir + '/glove6b/glove.6B.%sd.txt' % (embedding_dim),\n",
    "          encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "0QsnpPrNfhLP",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Get dense vector\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, i in (tokenizer.word_index).items():\n",
    "    # All 0 is words not found in the embedding index\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "QJPjW_PkfhLQ",
    "outputId": "9394799e-1d5a-4264-abda-915f031740bf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "embedding_matrix.shape"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TxvdWF3rfhLT"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6iiT1OoC1B8W"
   },
   "source": [
    "The original model framework used here is inspired by various online blog posts and tutorials on similar projects, but the code is largely my own. Some function definitions, such as generate sequence, come from Stack Overflow help articles on word-by-word predictions. The use of Inception-V3 and for image encoding is inspired by the paper \"Show, Attend and Tell: Neural Image Caption Generation with Visual Attention.\""
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "hQjPy1trfhLT",
    "outputId": "b5c37728-c71b-442b-e93d-2d1817dbd24c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Extract features\n",
    "inputs1 = tf.keras.Input(shape=(2048, ))\n",
    "fe1 = tf.keras.layers.Dropout(0.5)(inputs1)\n",
    "fe2 = tf.keras.layers.Dense(256, activation='relu')(fe1)\n",
    "# Sequence model\n",
    "inputs2 = tf.keras.Input(shape=(max_length, ))\n",
    "se1 = tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                                mask_zero=True)(inputs2)\n",
    "se2 = tf.keras.layers.Dropout(0.5)(se1)\n",
    "se3 = tf.keras.layers.LSTM(256)(se2)\n",
    "# Decoder model\n",
    "decoder1 = tf.keras.layers.Add()([fe2, se3])\n",
    "decoder2 = tf.keras.layers.Dense(256, activation='relu')(decoder1)\n",
    "outputs = tf.keras.layers.Dense(vocab_size, activation='softmax')(decoder2)\n",
    "# Final model\n",
    "model = tf.keras.models.Model(inputs=[inputs1, inputs2], outputs=outputs)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "by5sLAA41B8Z"
   },
   "source": [
    "Set weight using the glove embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "-p0_zneZfhLV",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Add the embedding matrix and set the layer to be not trainable\n",
    "model.layers[2].set_weights([embedding_matrix])\n",
    "model.layers[2].trainable = False"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "NV9ZBIjFfhLY",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "qVdRm5DBAqWg",
    "outputId": "b3bdb301-31eb-45e0-b214-3d0cba4f3e3c",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "model.summary()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dRJ3iqFeV-Sv"
   },
   "source": [
    "In this model, input layers from images and captions vectors are passed into the model, dropout layers are applied to the respective inputs, and then the text input is passed through an LSTM layer while the image input through a fully connected layer to ensure the output has the same dimension as the LSTM output. The input tensors are then added together and passed through fully connected layers for next word prediction. To train and predict the caption word-by-word, the model starts by using the current image and the starting tag “start_sentence” to predict the first actual word of the caption. After this iteration, two words are in the input. Based on the “start_sentence” and the last word predicted, the model is then trained to predict the second actual word of the caption. This cycle continues until the model has predicted the “end_sentence” word. I will now explain why dropout layers and LSTM layer are used. \n",
    "\n",
    "Dropout layers are used because they can reduce overfitting (regularizing), therefore improving the generalizability of the model. The dropout layer achieves this effect by randomly sample and drop outputs from the image and text input layers. Due to the limited number of images used, I decided to use a dropout layer because the large network used to train the model on this small data set has the potential of over-learning the signals in the images and text, picking up noise as signal as a result. A commonly used dropout value is 0.5, which means each output has 50% of the chance to be retained. I use a dropout value of 0.5. \n",
    "\n",
    "After passing the image and text features through the dropout layer respectively, I passed the text input through a Long Short Term Memory (LSTM) layer, and the image feature input through a dense layer, before adding the inputs together. When generating a sentence, creating a sensible prediction of the next word requires an understanding of the word prior to it. Therefore, a recurrent neural network is used when understanding of the prior data is required to produce more accurate prediction. While RNN is not competent at learning long-term dependencies, LSTM model is able to remember information for a long period of time. This is essential to the word-by-word prediction tasks because the last word of a caption might bear strong connection to the first word. For instance, in the sentence “a plane fly through the sky”, the last word “sky” ties back to the second word “a plane”, because the subject is a “plane”, it can only fly through the “sky” rather fly through, say, the “road”. In this case, an understanding of the long-term dependencies is needed to successfully make an accurate prediction of the last word. On the other hand, the image data is passed through a fully connected layer to shrink the image features to the same size as a text input. \n",
    "\n",
    "After processing image and text by themselves, the Add layer add the image and text tensors and return a single tensor before passing them through last two fully connected layers for prediction. Based on the input sequence and the image features, the final model predicts the output of the next word. I then append the new word predicted by the model onto the existing list of words, which then serve as the new input sequence to be supplied to the model along with the same image. This prediction cycle repeats until the model has predicted the keyword “end_sentence” to let us know that it believes this is the appropriate place to end the sentence. \n",
    "\n",
    "The model is trained on 80,000 image features together with their respective captions. The captions are broken down to sequence of input and output with each segment of the sentence as input. A sequence generator is used to take the first word as input to predict the second word, and then first and second words as input to predict the third word, and so on. The sequence generator is then placed inside the data generator to link the sequence inputs with the image inputs and yield these results to the fit generator of the model. The series of generators make it possible for the model to run through the word-by-word prediction on this large number of image vectors without running into memory issues. \n",
    "\n",
    "At the first few epochs, the model exhibits underfitting behavior where no distinct captions are generated, but after around 60 epochs, the model starts to exhibit overfitting behavior where convoluted vocabularies are used to form captions that do not make much sense. While training the model, I spent additional time to evaluate sample output in person rather than relying solely on metrics such as categorical loss and BLEU scores, because sometimes the captions produced by the model with the low loss and high BLEU scores do not always make sense to a human reader, and the appeal to human readers is the best judge of the model performance. \n",
    "\n",
    "A wide range of parameters in the model can be tuned. Due to the limited computation resources and methods for tuning due to the use of the fit generator, I tuned the batch size of 500, 1000, 2000, and 3000. The model with a batch size of 500 produced most sensible results overall among the 30 images I sampled. If I have more time, I will also tune the dropout rate, maximum sentence length, word frequency cut-off values (currently none), etc. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dkROK4u4fhLd"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rkNzQgdB1B8e"
   },
   "source": [
    "This function is used to generate a sequence of text as input and output. The generated sequences are then past into data generator and used for caption predictions. The `generate_sequences` function allows the model to perform word-by-word caption generation."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "SZvjq4PYfhLd",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Generate sentence to break sentence into in and out segments for word-by-word prediction\n",
    "def generate_sequences(tokenizer, max_length, caption, image_feature):\n",
    "    Ximages, XSeq, y = list(), list(), list()\n",
    "    vocab_size = len(tokenizer.word_index)\n",
    "    seq = tokenizer.texts_to_sequences([caption])[0]\n",
    "    # Split one sequence into multiple X,y pairs\n",
    "    for i in range(1, len(seq)):\n",
    "        # Select substrings\n",
    "        in_seq, out_seq = seq[:i], seq[i]\n",
    "        # Pad input sequence\n",
    "        in_seq = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [in_seq], maxlen=max_length, padding='post')[0]\n",
    "        # Encode output sequence\n",
    "        out_seq = tf.keras.utils.to_categorical([out_seq],\n",
    "                                                num_classes=vocab_size)[0]\n",
    "\n",
    "        image_feature = np.squeeze(image_feature)\n",
    "        Ximages.append(image_feature)\n",
    "        XSeq.append(in_seq)\n",
    "        y.append(out_seq)\n",
    "        \n",
    "    # Connect sentence sequence with images and the output sequence\n",
    "    Ximages, XSeq, y = np.array(Ximages), np.array(XSeq), np.array(y)\n",
    "    return [Ximages, XSeq, y]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B2FB0Few1B8h"
   },
   "source": [
    "Create Python data generator object to loop through all images. Use batch size to batch images and their corresponding caption sequences together before `yielding` to the `fit generator`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "FtfdLOcdfhLh",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Python data generator object to loop through all images\n",
    "def data_generator(tokenizer, max_length, captions, images_features, batch_size):\n",
    "    n = 0\n",
    "    while True:\n",
    "        for i in range(len(captions)):\n",
    "            in_img_vector = []\n",
    "            in_seq_vector = []\n",
    "            out_word_vector = []\n",
    "            # Load image feature\n",
    "            image_feature = images_features[i]\n",
    "            # Generate word sequence\n",
    "            caption = captions[i]\n",
    "            in_img, in_seq, out_word = generate_sequences(\n",
    "                tokenizer, max_length, caption, image_feature)\n",
    "            in_img_vector.append(in_img)\n",
    "            in_seq_vector.append(in_seq)\n",
    "            out_word_vector.append(out_word)\n",
    "            n += 1\n",
    "            # When batch size is reached, yield the output\n",
    "            if n == batch_size:\n",
    "                n = 0\n",
    "                yield [in_img, in_seq], out_word"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "KnsoCvd8fhLj",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Predict caption\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "    # seed the generation process\n",
    "    in_text = 'start_sentence'\n",
    "    for i in range(max_length):\n",
    "        # Use input and image to start predict the rest of the caption\n",
    "        sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "        sequence = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "            [sequence], maxlen=max_length, padding='post')\n",
    "        photo = photo.reshape(2048, 1).T\n",
    "        \n",
    "        # Predict the next word based on sequence and image\n",
    "        yhat = model.predict([photo, sequence], verbose=0)\n",
    "        yhat = np.argmax(yhat)\n",
    "        word = tokenizer.index_word[yhat]\n",
    "        \n",
    "        # End prediction when no word is predicted or if ending word is seen\n",
    "        if word is None:\n",
    "            break\n",
    "        in_text += ' ' + word\n",
    "        if word == 'end_sentence':\n",
    "            break\n",
    "    \n",
    "    # Remove beginning and ending signal words when output\n",
    "    in_text = re.sub(r'(start|end)_sentence', '', in_text).strip()\n",
    "    return in_text"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "BFdxaO4BfhLk",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Store default weights in the model directory\n",
    "model_directory = './model'\n",
    "\n",
    "if not os.path.exists(model_directory):\n",
    "    os.makedirs(model_directory)\n",
    "\n",
    "model.save_weights(model_directory + '/model.h5')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sZgMaE8M1B8n"
   },
   "source": [
    "Reset model weights to default"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "3wmnuLHE1B8n",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Reset model with default weights before training\n",
    "model.load_weights(model_directory + '/model.h5')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v6lJHxp51B8o"
   },
   "source": [
    "I used a fit generator because it is difficult to store $10,000$ images in the memory."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "bUNvLJ_GfhLn",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Specify final model parameters\n",
    "# After trying out different batch sizes, chose the one that provided the most reasonable results\n",
    "epochs = 50\n",
    "batch_size = 500\n",
    "steps = len(captions) // batch_size\n",
    "epoch_idx = 0"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ZyUwM5fq1B8t",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Create dictionary to store loss values. Need this because using fit generator\n",
    "loss = {}"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "30cfrCcS1B8v",
    "outputId": "ace1d277-6ed2-4d68-c148-a2001f447067",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Use fit generator due to limited memory.\n",
    "while epoch_idx <= epochs:\n",
    "    # Use data generator to input data\n",
    "    generator = data_generator(tokenizer, max_length, captions,\n",
    "                               images_features, batch_size)\n",
    "    # Fit generator is used due to memory limitation\n",
    "    history = model.fit_generator(generator, steps_per_epoch=steps, verbose=1)\n",
    "    loss[epoch_idx] = history.history['loss']\n",
    "    model.save_weights(model_directory + '/model_epoch%s_bs%s.h5' %\n",
    "                       (epoch_idx, batch_size))\n",
    "\n",
    "    epoch_idx += 1\n",
    "\n",
    "    # Keep track of the number of distinct captions\n",
    "    distinct_desc = []\n",
    "    temp_desc = ''\n",
    "    for j in range(20):\n",
    "        img_desc = generate_desc(model, tokenizer, images_features[j], max_length)\n",
    "        distinct_desc.append(img_desc)\n",
    "        temp_desc += '' + img_desc\n",
    "\n",
    "    print(\"Model %s generated %s distinct captions with %s distinct words.\" %\n",
    "          (epoch_idx, len(list(set(distinct_desc))), len(list(set(temp_desc.split(' '))))))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4yJWHqiV1B8y"
   },
   "source": [
    "At the first few epochs of the model training process, the model exhibits underfitting behavior where no distinct captions are generated. By understanding the number of distinct captions generated for a list of 20 images, I seek to approximate the appeal the model has for a human reader. In addition, as a model continues to learn from the training data, it becomes capable of generating more vocabularies as well."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "1e0C43df1B8y",
    "outputId": "3a49fae1-7899-4b0e-b339-bb4b73dc9217",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 232
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Return the loss plot\n",
    "plt.title('Model Training Loss')\n",
    "\n",
    "loss_lists = sorted(loss.items())\n",
    "x, y = zip(*loss_lists)\n",
    "plt.plot(x, y)\n",
    "\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zuH4h2P6WJJW"
   },
   "source": [
    "Here are some examples of the output from the model trained with a batch size of 500 after 50 epochs. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "lJQzcEjGfhLp",
    "colab": {},
    "outputId": "ec2b8462-1c69-4967-e361-9fa272e602b6",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# todo Add image transformation to improve learning\n",
    "# todo BLEU calculation here might be problematic\n",
    "# todo Calculate overall BLEU score\n",
    "# Train images\n",
    "print(\n",
    "    \"Sample prediction of the model with a batch size of %s and %s epochs.\\n\" %\n",
    "    (batch_size, epochs))\n",
    "\n",
    "# Load the latest model weight\n",
    "model.load_weights(model_directory + '/model_epoch%s_bs%s.h5' %\n",
    "                   (epochs, batch_size))\n",
    "\n",
    "# Output prediction for the first five images in the validation data. \n",
    "# Note that the data set is already shuffled.\n",
    "np.random.seed(101)\n",
    "for j in np.random.choice(range(len(test_captions)), 5):\n",
    "    # Compare captions\n",
    "    img_desc = generate_desc(model, tokenizer, test_images_features[j],\n",
    "                             max_length)\n",
    "    actual_caption = test_captions[j]\n",
    "    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption)\n",
    "\n",
    "    print(\"Predicted Caption: %s \\nActual Caption: %s\" %\n",
    "          (img_desc, actual_caption))\n",
    "    result_bleu = nltk.translate.bleu_score.sentence_bleu(actual_caption, \n",
    "                                                          img_desc)\n",
    "    print(\"Resulting BLEU-4 score is %s\" % (result_bleu))\n",
    "    # Show image\n",
    "    I = io.imread(test_images[j])\n",
    "    plt.imshow(I)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WInRtH_JfhLq"
   },
   "source": [
    "## Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YD4YvHEZ1B82"
   },
   "source": [
    "A wide range of parameters in the model can be tuned. Due to the limited computation resources and methods for tuning with fit generator, I tuned the batch size. If I have more time, I will also tune the drop out rate, maximum sentence length, word frequency cut-off values (currently none), etc."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fMb3QbteHUVb",
    "colab_type": "code",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Trying out different batch sizes\n",
    "for batch_size in [500, 1000, 2000, 3000]:\n",
    "    loss = {}\n",
    "    model.load_weights(model_directory + '/model.h5')\n",
    "    steps = len(captions) // batch_size\n",
    "    print(batch_size)\n",
    "    epoch_idx = 0\n",
    "    while epoch_idx <= epochs:\n",
    "        generator = data_generator(tokenizer, max_length, captions,\n",
    "                                   images_features, batch_size)\n",
    "        history = model.fit_generator(generator,\n",
    "                                      steps_per_epoch=steps,\n",
    "                                      verbose=1)\n",
    "        loss[epoch_idx] = history.history['loss']\n",
    "\n",
    "        model.save_weights(model_directory + '/model_epoch' + str(epoch_idx) +\n",
    "                           \"_bs\" + str(batch_size) + '.h5')\n",
    "        epoch_idx += 1\n",
    "\n",
    "        distinct_desc = []\n",
    "        temp_desc = ''\n",
    "        for j in range(20):\n",
    "            img_desc = generate_desc(model, tokenizer, images_features[j],\n",
    "                                     max_length)\n",
    "            distinct_desc.append(img_desc)\n",
    "            temp_desc += '' + img_desc\n",
    "\n",
    "        print(\"%s distinct captions, %s distinct words.\" % (len(\n",
    "            list(set(distinct_desc))), len(list(set(temp_desc.split(' '))))))\n",
    "\n",
    "    # Plot model loss\n",
    "    plt.title('Model Training Loss')\n",
    "\n",
    "    loss_lists = sorted(\n",
    "        loss.items())  # sorted by key, return a list of tuples\n",
    "    x, y = zip(*loss_lists)  # unpack a list of pairs into two tuples\n",
    "    plt.plot(x, y)\n",
    "\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.show()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rx4u6gyT1B83"
   },
   "source": [
    "Now, we turn to test the performance of the models trained using different batch sizes using the same train and validation images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "diAcxjO1fhLu"
   },
   "source": [
    "### Test on Training Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "E8jE4yGsfhLv",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "np.random.seed(17)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "Vh7FCmC61B85",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "rand_train_image = np.random.choice(range(len(captions)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "vT84EevvfhLz",
    "colab": {},
    "outputId": "da6615b4-3743-4d0f-cfe0-13fe06bd4ddd",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "I = io.imread(images[rand_train_image])\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Obtain actual and predicted images and their captions\n",
    "actual_caption = captions[rand_train_image]\n",
    "actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption).strip()\n",
    "print(\"The actual caption is:\\n%s \\n\\nThe predicted captions are:\" %\n",
    "      (actual_caption))\n",
    "\n",
    "for bs in [500, 1000, 2000, 3000]:\n",
    "    model.load_weights(model_directory + '/model_epoch%s_bs%s.h5' %\n",
    "                       (epochs - 1, bs))\n",
    "    # Compare captions\n",
    "    img_desc = generate_desc(model, tokenizer,\n",
    "                             images_features[rand_train_image], max_length)\n",
    "\n",
    "    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption)\n",
    "    print(\"Batch size = %s: %s\" % (bs, img_desc))\n",
    "    \n",
    "    result_bleu = nltk.translate.bleu_score.sentence_bleu(\n",
    "        actual_caption, img_desc)\n",
    "    print(\"Resulting BLEU-4 score is %s\\n\" % (result_bleu))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s5BvNKC3fhL0"
   },
   "source": [
    "### Test on Validation Data"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "owumeYKu1B88",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "np.random.seed(511)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "cRE0Vp3LfhL1",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "rand_test_image = np.random.choice(range(len(test_captions)))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "JwxqDNxyfhL2",
    "colab": {},
    "outputId": "0975493f-0917-453a-8512-90ddee4287f9",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "I = io.imread(images[rand_test_image])\n",
    "plt.imshow(I)\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "# Obtain actual and predicted images and their captions\n",
    "actual_caption = captions[rand_test_image]\n",
    "actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption).strip()\n",
    "print(\"The actual caption is:\\n%s \\n\\nThe predicted captions are:\" %\n",
    "      (actual_caption))\n",
    "\n",
    "for bs in [500, 1000, 2000, 3000]:\n",
    "    model.load_weights(model_directory + '/model_epoch%s_bs%s.h5' %\n",
    "                       (epochs - 1, bs))\n",
    "    # Compare captions\n",
    "    img_desc = generate_desc(model, tokenizer,\n",
    "                             images_features[rand_test_image], max_length)\n",
    "\n",
    "    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption)\n",
    "    print(\"Batch size = %s: %s\" % (bs, img_desc))\n",
    "    \n",
    "    result_bleu = nltk.translate.bleu_score.sentence_bleu(\n",
    "        actual_caption, img_desc)\n",
    "    print(\"Resulting BLEU-4 score is %s\\n\" % (result_bleu))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sjzB24t2fhL4"
   },
   "source": [
    "# Discussion"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "CFrE1tqh6J9e",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Understand vocab distribution to showcase the model bias\n",
    "# Note that identifying stop words and lemmatization takes a while.\n",
    "word_list = []\n",
    "for caption in captions:\n",
    "    seq = caption.split(\" \")\n",
    "    word_list += seq\n",
    "\n",
    "# Remove idiosyncracies in word usage\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "word_list = [\n",
    "    lemmatizer.lemmatize(word.lower()) for word in word_list\n",
    "    if word and word.lower() not in nltk.corpus.stopwords.words('english')\n",
    "    and \"_\" not in word\n",
    "]\n",
    "\n",
    "# Calculate frequency distribution\n",
    "word_list_dist = nltk.FreqDist(word_list)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "5aS8UhXm1B9B",
    "colab": {},
    "outputId": "850264dc-fc9e-4f62-87e1-909556d6614e",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Run this line to output most frequently used vocabularies in the data set\n",
    "limit = 30\n",
    "word_dict = dict()\n",
    "for word, frequency in word_list_dist.most_common(limit):\n",
    "    print(u'{}\\t\\t{}'.format(word, frequency))\n",
    "    word_dict[word] = frequency"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5IvRoq401B9C"
   },
   "source": [
    "## Learning\n",
    "Image captioning requires implementation of deep learning methods in both computer vision and natural language processing. While doing this project, I learned to use transfer learning by using image and word embeddings. Drop-out layers are included to prevent the model from overfitting, and model tuning is performed to ensure the model achieve reasonable performance.\n",
    "\n",
    "Model tuning helps identify that the model tends to perform better with the batch size of 500 among the list of the values tested. Increasing the number of epochs is shown to help the model generate more complex vocabularies but doing that also cause the model to overfit easily. The final model is able to produce sensible captions for the images supplied. However, there remains potential for improvements.\n",
    "\n",
    "## Areas of Improvement\n",
    "\n",
    "The model is able to provide reasonable captions given the images supplied, but the model appears to have difficulty distinguishing objects with similar property, and the model also has a bias towards objects and captions that appear more often in the data set. Last but not the least, a better metric should be used to systematically measure model performance.\n",
    "\n",
    "The model seems to have trouble distinguishing between objects that have subtle differences, such as sky and ocean, snowboards and surf board, etc. For instance, the model mistakes a man performing a snowboard jump as someone surfing in the ocean, presumably because the blue sky in the background bears resemblance to the color ocean, and the skis appearing in the image looks similar to a surf board. Data augmentation using data generators may help model better distinguish similar objects.\n",
    "\n",
    "Furthermore, there appear to be bias in captioning due to the class imbalance in the training data set. When describing people in images, the model is more likely to predict a person as a “man” or a \"guy\" even when the subject is in fact a woman. This is likely caused by the nature of the original training images and captions. As an evidence, in the “Word Distribution” section, you will notice that the “man” is the most popular word used after removing stop words, while women is ranked 5th, showing that male images occur more often than female's. Here is the short list of words with their respective frequencies in the data set together with an example of the prediction that shows the model's gender bias.\n",
    "\n",
    "\n",
    "```\n",
    "man        48873\n",
    "sitting    35724\n",
    "two        32401\n",
    "standing   28438\n",
    "people     27452\n",
    "woman      26082\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DsnmpgyF1B9C"
   },
   "source": [
    "In this example, the model confuses man and woman. As shown in the list above, \"man\" is the more popular word."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "HUfiZRoh1B9D",
    "colab": {},
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "def show_img_example(img_idx):\n",
    "    # Show images given index\n",
    "    I = io.imread(images[img_idx])\n",
    "    plt.imshow(I)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "    # Create actual and predicted caption\n",
    "    actual_caption = captions[img_idx]\n",
    "    actual_caption = re.sub(r'(start|end)_sentence', '', actual_caption).strip()\n",
    "\n",
    "    model.load_weights(model_directory + '/model_epoch50_bs500.h5')\n",
    "    img_desc = generate_desc(model, tokenizer, images_features[img_idx],\n",
    "                             max_length)\n",
    "\n",
    "    print(\"The actual caption is:\\n%s \\n\\nThe predicted captions is:\\n%s\" %\n",
    "          (actual_caption, img_desc))\n",
    "    \n",
    "    result_bleu = nltk.translate.bleu_score.sentence_bleu(\n",
    "        actual_caption, img_desc)\n",
    "    print(\"\\nResulting BLEU-4 score is %s\\n\" % (result_bleu))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "lzxb9ATc1B9D",
    "colab": {},
    "outputId": "b8d61a45-25b8-4c19-f1b8-a2cdbfb67d0d",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "show_img_example(9836)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VSpMhGZh1B9K"
   },
   "source": [
    "One future improvement is to up-sample the female images and captions in the data set to improve the model's ability to recognize female. In addition, in the process of image labeling, the generic use of “man” when both genders appear should be avoided to help model better understand the gender differences.\n",
    "\n",
    "The categorical loss measure for model accuracy is not a good measure of the model performance. While the model performance improves with categorical accuracy, at a low loss value, the model has tendency of overfitting, producing vocabularies that do not fit well together as a sentence. The categorical loss fails to account for the readability of the content. I have tested out the use of BLEU score to measure caption similarity to the original caption, but BLEU score fails to account for the semantic similarity of the predicted caption and actual caption. In addition, due to the difficulty with the caption learning task in itself, the scale BLEU score provides fails to provide meaningful comparison across models. For instance, one might argue that the predicted caption “a baby giraffe eating leaves on a meadow” is a good approximation of the original one “A giraffe that is eating some leaves off of a tree”. However, a BLEU score of $1.39e^{-231}$ is given in this case."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "OS1bkMjG1B9L",
    "colab": {},
    "outputId": "492b618c-9e74-4277-ed0d-625de70d2dac",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "result_bleu = nltk.translate.bleu_score.sentence_bleu(\n",
    "    \"A giraffe that is eating some leaves off of a tree.\",\n",
    "    \"a baby giraffe eating leaves on a meadow \")\n",
    "print(\"Resulting BLEU-4 score is %s\" % (result_bleu))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r_Vemxdo1B9M"
   },
   "source": [
    "Lastly, I was unable to identify a way to perform grid search for hyperparameter tuning when fit_generator is used. In addition, my computation resource is limited. Therefore, I used a for loops for testing the model with different batch sizes. If time and resource permit, I will also tune dropout rate, maximum sentence length, word frequency cut-off values (currently none), etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I2FeC2W61B9M"
   },
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "irgaQ3sG1B9M"
   },
   "source": [
    "Random image generator to help me find examples:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab_type": "code",
    "id": "ZW1Witgo1B9N",
    "colab": {},
    "outputId": "a1858ad0-ff78-4559-893b-e41f988874e5",
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "img_idx = np.random.choice(range(len(test_captions)))\n",
    "show_img_example(img_idx)\n",
    "print(img_idx)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HK3pRGCA1B9O"
   },
   "source": [
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ydw0jOWb1B9O"
   },
   "source": [
    "Link to Youtube Video: https://youtu.be/_IFIf4Gn4i0"
   ]
  }
 ]
}